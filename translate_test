import json
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_rel, wilcoxon
from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances
from sklearn.decomposition import PCA
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
import warnings
import gc
import psutil
import time
warnings.filterwarnings('ignore')

class MultilingualLayerAnalysis:
    def __init__(self, model_name_or_path, max_length=512, device='cuda'):
        """
        初始化多语言层间分析工具
        
        Args:
            model_name_or_path: 预训练模型路径
            max_length: 最大序列长度
            device: 设备
        """
        self.device = device
        self.max_length = max_length
        
        # 设置多GPU环境
        if torch.cuda.is_available():
            self.device = 'cuda'
            self.num_gpus = torch.cuda.device_count()
            print(f"检测到 {self.num_gpus} 个GPU")
            
            # 显示每个GPU的信息
            for i in range(self.num_gpus):
                gpu_props = torch.cuda.get_device_properties(i)
                print(f"GPU {i}: {gpu_props.name}, 显存: {gpu_props.total_memory / 1024**3:.1f}GB")
        else:
            self.device = 'cpu'
            self.num_gpus = 0
            print("使用CPU运行")
        
        # 加载模型和分词器
        print("加载模型和分词器...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name_or_path, 
                trust_remote_code=True,
                padding_side='left'  # Qwen模型推荐使用左填充
            )
            
            # 设置pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            # 优化模型加载配置
            model_kwargs = {
                'output_hidden_states': True,
                'trust_remote_code': True
            }
            
            if torch.cuda.is_available():
                model_kwargs.update({
                    'torch_dtype': torch.float16,  # 使用半精度节省显存
                    'device_map': 'auto',  # 自动分配到多GPU
                    'low_cpu_mem_usage': True,  # 减少CPU内存使用
                })
            else:
                model_kwargs['torch_dtype'] = torch.float32
                
            self.model = AutoModel.from_pretrained(
                model_name_or_path,
                **model_kwargs
            )
            
            self.model.eval()
            
            # 获取层数 - 改进的兼容性逻辑
            if hasattr(self.model.config, 'num_hidden_layers'):
                self.num_layers = self.model.config.num_hidden_layers
            elif hasattr(self.model.config, 'num_layers'):
                self.num_layers = self.model.config.num_layers
            elif hasattr(self.model.config, 'n_layer'):
                self.num_layers = self.model.config.n_layer
            elif hasattr(self.model.config, 'n_layers'):
                self.num_layers = self.model.config.n_layers
            else:
                # 尝试通过推理或手动计算获取层数
                try:
                    # 使用一个简单的输入来获取hidden_states的数量
                    print("配置中未找到层数信息，通过推理获取...")
                    dummy_input = self.tokenizer("test", return_tensors="pt", padding=True, truncation=True)
                    if torch.cuda.is_available():
                        dummy_input = {k: v.cuda() for k, v in dummy_input.items()}
                    
                    with torch.no_grad():
                        outputs = self.model(**dummy_input, output_hidden_states=True)
                        # 改进的层数推断逻辑，处理不同模型格式
                        if len(outputs.hidden_states) > 2:
                            self.num_layers = len(outputs.hidden_states) - 2  # 去掉embedding和最终norm
                        else:
                            self.num_layers = len(outputs.hidden_states) - 1
                    
                    # 清理推理时的GPU内存
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        
                except Exception as e:
                    print(f"无法通过推理获取层数，尝试手动计算: {e}")
                    # 手动计算层数
                    self.num_layers = len([name for name, _ in self.model.named_modules() if 'layer' in name and 'norm' not in name])
                    if self.num_layers == 0:
                        print("手动计算层数失败，使用默认值")
                        self.num_layers = 28  # 默认值
            
            print(f"模型加载成功，层数: {self.num_layers}")
            
            # 初始化GPU内存监控
            if torch.cuda.is_available():
                self._init_gpu_monitoring()
            
        except Exception as e:
            print(f"模型加载失败: {e}")
            raise e
        
        # 存储结果
        self.layer_similarities = {}
        self.mmd_results = {}
    
    def _init_gpu_monitoring(self):
        """初始化GPU内存监控"""
        self.initial_gpu_memory = {}
        for i in range(self.num_gpus):
            torch.cuda.set_device(i)
            torch.cuda.empty_cache()
            self.initial_gpu_memory[i] = torch.cuda.memory_allocated(i)
            print(f"GPU {i} 初始内存使用: {self.initial_gpu_memory[i] / 1024**3:.2f}GB")
    
    def _cleanup_gpu_memory(self):
        """清理GPU内存"""
        if torch.cuda.is_available():
            # 强制垃圾回收
            gc.collect()
            
            # 清空所有GPU缓存
            for i in range(self.num_gpus):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # 等待内存释放
            import time
            time.sleep(1)
    
    def _get_optimal_batch_size(self, base_batch_size=8):
        """根据GPU显存和模型大小动态计算最优批大小"""
        if not torch.cuda.is_available():
            return base_batch_size
        
        # 获取当前GPU显存信息
        current_device = torch.cuda.current_device()
        gpu_props = torch.cuda.get_device_properties(current_device)
        total_memory = gpu_props.total_memory / 1024**3  # GB
        allocated_memory = torch.cuda.memory_allocated(current_device) / 1024**3  # GB
        cached_memory = torch.cuda.memory_reserved(current_device) / 1024**3  # GB
        available_memory = total_memory - max(allocated_memory, cached_memory)
        
        # 估算模型参数大小（更精确）
        model_params = sum(p.numel() for p in self.model.parameters())
        # 根据模型精度调整内存估算
        if hasattr(self.model, 'dtype'):
            if self.model.dtype == torch.float16 or self.model.dtype == torch.half:
                bytes_per_param = 2  # float16
            elif self.model.dtype == torch.bfloat16:
                bytes_per_param = 2  # bfloat16
            elif self.model.dtype == torch.float8_e4m3fn or self.model.dtype == torch.float8_e5m2:
                bytes_per_param = 1  # float8
            else:
                bytes_per_param = 4  # float32
        else:
            # 默认假设为float16
            bytes_per_param = 2
            
        model_size_gb = model_params * bytes_per_param / 1024**3
        
        # 更精确地估算每个样本的内存需求
        hidden_size = getattr(self.model.config, 'hidden_size', 4096)
        # 检查模型架构以更准确估计内存需求
        if hasattr(self.model.config, 'model_type'):
            model_type = self.model.config.model_type.lower()
            # 不同模型架构的内存需求系数
            if 'bert' in model_type or 'roberta' in model_type:
                memory_factor = 3.0  # BERT类模型
            elif 'gpt' in model_type or 'llama' in model_type or 'qwen' in model_type:
                memory_factor = 4.0  # GPT类自回归模型
            elif 't5' in model_type:
                memory_factor = 5.0  # Encoder-Decoder模型
            else:
                memory_factor = 4.0  # 默认系数
        else:
            memory_factor = 4.0  # 默认系数
            
        # 考虑激活值、梯度和优化器状态
        sample_memory_mb = (self.max_length * hidden_size * self.num_layers * bytes_per_param * memory_factor) / 1024**2
        
        # 动态安全系数：根据可用内存和模型大小调整
        if model_size_gb > available_memory * 0.5:
            # 大模型相对于可用内存，使用更保守的安全系数
            safety_factor = 0.5
        elif available_memory < 8:
            # 小显存环境，更保守
            safety_factor = 0.6
        else:
            # 标准情况
            safety_factor = 0.7
            
        # 计算理论最大批大小
        max_batch_memory = available_memory * safety_factor * 1024  # MB
        theoretical_max_batch = max(1, int(max_batch_memory / sample_memory_mb))
        
        # 根据不同显存大小设置批大小策略，但考虑模型复杂度
        if available_memory > 30:  # 大显存GPU (>30GB)
            optimal_batch_size = min(theoretical_max_batch, 32)
        elif available_memory > 20:  # 中等显存GPU (20-30GB)
            optimal_batch_size = min(theoretical_max_batch, 16)
        elif available_memory > 10:  # 普通显存GPU (10-20GB)
            optimal_batch_size = min(theoretical_max_batch, 8)
        elif available_memory > 5:   # 小显存GPU (5-10GB)
            optimal_batch_size = min(theoretical_max_batch, 4)
        else:  # 极小显存GPU (<5GB)
            optimal_batch_size = 1
        
        # 考虑模型大小对批大小的影响
        if model_size_gb > 10:  # 超大模型
            optimal_batch_size = min(optimal_batch_size, 4)
        elif model_size_gb > 5:  # 大模型
            optimal_batch_size = min(optimal_batch_size, 8)
            
        # 确保批大小在合理范围内
        optimal_batch_size = min(optimal_batch_size, base_batch_size * 2)
        optimal_batch_size = max(optimal_batch_size, 1)  # 至少为1
        
        print(f"动态批大小计算: 可用显存={available_memory:.2f}GB, 模型大小={model_size_gb:.2f}GB, 安全系数={safety_factor}, 批大小={optimal_batch_size}")
        
        print(f"GPU显存分析:")
        print(f"  总计: {total_memory:.1f}GB")
        print(f"  已分配: {allocated_memory:.1f}GB")
        print(f"  已缓存: {cached_memory:.1f}GB") 
        print(f"  可用: {available_memory:.1f}GB")
        print(f"  模型大小: {model_size_gb:.1f}GB")
        print(f"  每样本内存需求: {sample_memory_mb:.1f}MB")
        print(f"  理论最大批大小: {theoretical_max_batch}")
        print(f"  优化批大小: {optimal_batch_size}")
        
        return optimal_batch_size
    
    def _adaptive_batch_size_adjustment(self, current_batch_size, error_type="oom"):
        """自适应批大小调整策略"""
        if error_type == "oom":  # Out of Memory
            # GPU内存不足时的调整策略
            if current_batch_size > 8:
                new_batch_size = current_batch_size // 2
            elif current_batch_size > 4:
                new_batch_size = current_batch_size - 2
            elif current_batch_size > 1:
                new_batch_size = current_batch_size - 1
            else:
                new_batch_size = 1
        else:  # 其他错误
            # 保守调整
            new_batch_size = max(1, current_batch_size - 1)
        
        return new_batch_size
    
    def _monitor_gpu_utilization(self):
        """监控GPU利用率和内存使用情况"""
        if not torch.cuda.is_available():
            return {}
        
        gpu_stats = {}
        for i in range(self.num_gpus):
            torch.cuda.set_device(i)
            
            # 内存统计
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            
            gpu_stats[f'gpu_{i}'] = {
                'allocated_gb': allocated,
                'cached_gb': cached,
                'total_gb': total,
                'utilization_percent': (allocated / total) * 100
            }
        
        return gpu_stats

    def load_parallel_data(self, data_path, sample_size=None):
        """
        加载并行语料数据
        
        Args:
            data_path: 数据文件路径
            sample_size: 采样大小，默认为None表示使用全部数据
        """
        print("加载并行语料...")
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"数据加载失败: {e}")
            raise e
        
        # 提取并行句对
        parallel_pairs = []
        for item in data:
            try:
                if 'content' in item:
                    # 处理content是列表的情况
                    contents = item['content']
                    if isinstance(contents, list):
                        for content in contents:
                            if '1' in content and '2' in content:
                                uyghur_text = content['1'].strip()
                                chinese_text = content['2'].strip()
                                # 过滤掉太短或太长的句子
                                if (uyghur_text and chinese_text and 
                                    10 <= len(uyghur_text) <= 500 and 
                                    10 <= len(chinese_text) <= 500):
                                    parallel_pairs.append({
                                        'uyghur': uyghur_text,
                                        'chinese': chinese_text
                                    })
                    # 处理content是字典的情况
                    elif isinstance(contents, dict) and '1' in contents and '2' in contents:
                        uyghur_text = contents['1'].strip()
                        chinese_text = contents['2'].strip()
                        if (uyghur_text and chinese_text and 
                            10 <= len(uyghur_text) <= 500 and 
                            10 <= len(chinese_text) <= 500):
                            parallel_pairs.append({
                                'uyghur': uyghur_text,
                                'chinese': chinese_text
                            })
            except Exception as e:
                print(f"处理数据项时出错: {e}")
                continue
        
        print(f"有效并行句对: {len(parallel_pairs)}")
        
        if len(parallel_pairs) == 0:
            raise ValueError("未找到有效的并行句对，请检查数据格式")
        
        # 随机采样（仅当sample_size不为None且小于总数时）
        if sample_size is not None and len(parallel_pairs) > sample_size:
            np.random.seed(42)
            indices = np.random.choice(len(parallel_pairs), sample_size, replace=False)
            parallel_pairs = [parallel_pairs[i] for i in indices]
        
        self.parallel_data = parallel_pairs
        print(f"最终使用 {len(parallel_pairs)} 对并行句")
        
        # 打印数据样例
        print("数据样例:")
        for i, pair in enumerate(parallel_pairs[:3]):
            print(f"样例{i+1}:")
            print(f"  维语: {pair['uyghur'][:100]}...")
            print(f"  中文: {pair['chinese'][:100]}...")
        
        return parallel_pairs
    
    def extract_sentence_representations(self, texts, batch_size=None):
        """
        提取句子在各层的表示 - 优化GPU并行计算版本
        
        Args:
            texts: 文本列表
            batch_size: 批大小，如果为None则自动计算最优值
            
        Returns:
            layer_representations: {layer_id: [sentence_vectors]}
        """
        layer_representations = {i: [] for i in range(self.num_layers)}
        
        # 自动计算最优批大小
        if batch_size is None:
            batch_size = self._get_optimal_batch_size()
        
        print(f"提取句子表示 (批大小: {batch_size}, 文本数量: {len(texts)})...")
        
        # 检测模型架构类型
        model_type = getattr(self.model.config, 'model_type', '').lower()
        is_encoder_only = any(arch in model_type for arch in ['bert', 'roberta', 'electra', 'deberta'])
        
        print(f"模型类型: {model_type}, 使用{'CLS token' if is_encoder_only else 'Last token'}策略")
        
        # 初始化错误重试机制
        max_retries = 3
        current_batch_size = batch_size
        processed_batches = 0
        failed_batches = 0
        
        # 预处理：按长度排序以优化批处理效率
        text_with_indices = [(i, text) for i, text in enumerate(texts)]
        text_with_indices.sort(key=lambda x: len(x[1]))
        
        # 创建结果存储，保持原始顺序
        temp_layer_representations = {i: [None] * len(texts) for i in range(self.num_layers)}
        
        progress_bar = tqdm(total=len(text_with_indices), desc="处理文本")
        
        for batch_start in range(0, len(text_with_indices), current_batch_size):
            batch_end = min(batch_start + current_batch_size, len(text_with_indices))
            batch_items = text_with_indices[batch_start:batch_end]
            batch_indices = [item[0] for item in batch_items]
            batch_texts = [item[1] for item in batch_items]
            
            retry_count = 0
            batch_processed = False
            
            while retry_count < max_retries and not batch_processed:
                try:
                    # 分词
                    inputs = self.tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=self.max_length,
                        return_tensors='pt'
                    )
                    
                    # 将输入移动到GPU
                    if torch.cuda.is_available():
                        inputs = {k: v.to(self.device, non_blocking=True) for k, v in inputs.items()}
                    
                    # 前向传播
                    with torch.no_grad():
                        # 启用混合精度计算
                        if torch.cuda.is_available():
                            with torch.cuda.amp.autocast():
                                outputs = self.model(**inputs)
                        else:
                            outputs = self.model(**inputs)
                            
                        hidden_states = outputs.hidden_states
                        
                        # 对每一层提取句向量
                        for layer_idx in range(self.num_layers):
                            layer_hidden = hidden_states[layer_idx + 1]  # 跳过embedding层
                            
                            if is_encoder_only:
                                # BERT-like模型：使用CLS token
                                sentence_vectors = layer_hidden[:, 0, :]
                            else:
                                # Decoder-only模型：使用最后一个非pad token
                                seq_lengths = inputs['attention_mask'].sum(dim=1) - 1
                                sentence_vectors = layer_hidden[torch.arange(layer_hidden.size(0)), seq_lengths]
                            
                            # 转换为CPU并存储到正确位置
                            vectors_cpu = sentence_vectors.cpu().float().numpy()
                            for i, orig_idx in enumerate(batch_indices):
                                temp_layer_representations[layer_idx][orig_idx] = vectors_cpu[i]
                    
                    batch_processed = True
                    processed_batches += 1
                    progress_bar.update(len(batch_texts))
                    
                    # 定期清理GPU内存
                    if processed_batches % 10 == 0:
                        self._cleanup_gpu_memory()
                        
                except torch.cuda.OutOfMemoryError as e:
                    retry_count += 1
                    failed_batches += 1
                    
                    print(f"\nGPU内存不足 (重试 {retry_count}/{max_retries}): {e}")
                    
                    # 清理内存
                    self._cleanup_gpu_memory()
                    
                    # 减小批大小
                    current_batch_size = max(1, current_batch_size // 2)
                    print(f"减小批大小到: {current_batch_size}")
                    
                    # 如果批大小已经是1，跳过这个批次
                    if current_batch_size == 1 and retry_count >= max_retries:
                        print(f"跳过批次 {batch_start}-{batch_end}，批大小已最小")
                        break
                        
                except Exception as e:
                    retry_count += 1
                    print(f"\n处理批次时出错 (重试 {retry_count}/{max_retries}): {e}")
                    
                    if retry_count >= max_retries:
                        print(f"跳过批次 {batch_start}-{batch_end}")
                        break
        
        progress_bar.close()
        
        # 转换为最终格式，过滤掉None值
        for layer_idx in range(self.num_layers):
            valid_vectors = [v for v in temp_layer_representations[layer_idx] if v is not None]
            if valid_vectors:
                layer_representations[layer_idx] = np.array(valid_vectors)
            else:
                print(f"警告: 第{layer_idx}层没有提取到有效表示")
                layer_representations[layer_idx] = np.array([])
        
        print(f"\n处理完成: 成功批次 {processed_batches}, 失败批次 {failed_batches}")
        
        # 最终清理
        self._cleanup_gpu_memory()
        
        return layer_representations

    def compute_layer_similarities(self):
        """
        计算层间相似度 - 带GPU性能监控版本
        """
        print("计算层间相似度...")
        
        # 开始性能监控
        start_time = time.time()
        initial_gpu_stats = self._monitor_gpu_utilization()
        
        # 提取维语和中文句子表示
        uyghur_texts = [pair['uyghur'] for pair in self.parallel_data]
        chinese_texts = [pair['chinese'] for pair in self.parallel_data]
        
        print(f"开始提取维语表示 ({len(uyghur_texts)} 个句子)...")
        uyghur_start_time = time.time()
        uyghur_representations = self.extract_sentence_representations(uyghur_texts)
        uyghur_time = time.time() - uyghur_start_time
        
        # 监控中间状态
        mid_gpu_stats = self._monitor_gpu_utilization()
        self._print_gpu_stats("维语处理后", mid_gpu_stats)
        
        print(f"开始提取中文表示 ({len(chinese_texts)} 个句子)...")
        chinese_start_time = time.time()
        chinese_representations = self.extract_sentence_representations(chinese_texts)
        chinese_time = time.time() - chinese_start_time
        
        # 监控最终状态
        final_gpu_stats = self._monitor_gpu_utilization()
        self._print_gpu_stats("中文处理后", final_gpu_stats)
        
        # 计算每层的余弦相似度
        print("计算层间余弦相似度...")
        similarity_start_time = time.time()
        results = []
        
        for layer_idx in range(self.num_layers):
            if (layer_idx in uyghur_representations and 
                layer_idx in chinese_representations and
                len(uyghur_representations[layer_idx]) > 0 and
                len(chinese_representations[layer_idx]) > 0):
                
                uy_vectors = uyghur_representations[layer_idx]
                zh_vectors = chinese_representations[layer_idx]
                
                # 确保向量数量匹配
                min_len = min(len(uy_vectors), len(zh_vectors))
                uy_vectors = uy_vectors[:min_len]
                zh_vectors = zh_vectors[:min_len]
                
                # 计算配对的余弦相似度
                similarities = []
                for i in range(min_len):
                    # 检查向量中是否有无穷大或NaN值
                    uy_vec = uy_vectors[i]
                    zh_vec = zh_vectors[i]
                    
                    # 替换无穷大和NaN值
                    uy_vec = np.nan_to_num(uy_vec, nan=0.0, posinf=1.0, neginf=-1.0)
                    zh_vec = np.nan_to_num(zh_vec, nan=0.0, posinf=1.0, neginf=-1.0)
                    
                    # 计算余弦相似度
                    try:
                        sim = cosine_similarity([uy_vec], [zh_vec])[0, 0]
                    except Exception as e:
                        print(f"计算相似度时出错: {e}, 使用默认值0")
                        sim = 0.0
                        
                    similarities.append(sim)
                
                similarities = np.array(similarities)
                mean_sim = np.mean(similarities)
                std_sim = np.std(similarities)
                
                # 95% 置信区间
                ci_lower = np.percentile(similarities, 2.5)
                ci_upper = np.percentile(similarities, 97.5)
                
                results.append({
                    'layer': layer_idx,
                    'mean_cosine': mean_sim,
                    'std_cosine': std_sim,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper,
                    'similarities': similarities,
                    'sample_count': len(similarities)
                })
            else:
                print(f"警告: 第{layer_idx}层数据不完整，跳过")
        
        similarity_time = time.time() - similarity_start_time
        total_time = time.time() - start_time
        
        # 性能统计
        print(f"\n=== 性能统计 ===")
        print(f"维语表示提取时间: {uyghur_time:.2f}秒")
        print(f"中文表示提取时间: {chinese_time:.2f}秒")
        print(f"相似度计算时间: {similarity_time:.2f}秒")
        print(f"总处理时间: {total_time:.2f}秒")
        print(f"平均每层处理时间: {total_time/self.num_layers:.2f}秒")
        
        # GPU利用率对比
        self._compare_gpu_stats("初始状态", initial_gpu_stats, "最终状态", final_gpu_stats)
        
        self.layer_similarities = results
        
        # 计算统计显著性
        self.compute_similarity_significance()
        
        # 最终清理
        self._cleanup_gpu_memory()
        
        return results
    
    def _print_gpu_stats(self, stage_name, gpu_stats):
        """打印GPU统计信息"""
        if not gpu_stats:
            return
            
        print(f"\n--- {stage_name} GPU状态 ---")
        for gpu_id, stats in gpu_stats.items():
            print(f"{gpu_id}: 已用 {stats['allocated_gb']:.2f}GB/"
                  f"{stats['total_gb']:.1f}GB ({stats['utilization_percent']:.1f}%)")
    
    def _compare_gpu_stats(self, stage1_name, stats1, stage2_name, stats2):
        """比较两个阶段的GPU统计信息"""
        if not stats1 or not stats2:
            return
            
        print(f"\n=== GPU利用率对比: {stage1_name} vs {stage2_name} ===")
        for gpu_id in stats1.keys():
            if gpu_id in stats2:
                initial = stats1[gpu_id]
                final = stats2[gpu_id]
                memory_diff = final['allocated_gb'] - initial['allocated_gb']
                util_diff = final['utilization_percent'] - initial['utilization_percent']
                
                print(f"{gpu_id}:")
                print(f"  内存变化: {memory_diff:+.2f}GB")
                print(f"  利用率变化: {util_diff:+.1f}%")

    def compute_similarity_significance(self):
        """
        计算层间相似度的统计显著性，使用更稳健的方法
        """
        print("计算统计显著性...")
        
        # 选择更多的关键层进行比较，增加比较点以获得更精确的置信区间
        key_layers = [0, self.num_layers // 4, self.num_layers // 2, 
                     3 * self.num_layers // 4, self.num_layers - 1]
        
        for result in self.layer_similarities:
            layer_idx = result['layer']
            similarities = result['similarities']
            
            # 增强的数据预处理：更稳健地处理极端值和缺失值
            similarities = np.array(similarities)
            
            # 使用中位数替换nan值，并处理无穷值
            median_val = np.nanmedian(similarities)
            similarities = np.nan_to_num(similarities, nan=median_val, 
                                        posinf=np.nanmax(similarities[np.isfinite(similarities)]) if np.any(np.isfinite(similarities)) else 1.0, 
                                        neginf=np.nanmin(similarities[np.isfinite(similarities)]) if np.any(np.isfinite(similarities)) else -1.0)
            
            # 更稳健的异常值检测 - 使用调整后的IQR方法
            q1 = np.percentile(similarities, 25)
            q3 = np.percentile(similarities, 75)
            iqr = q3 - q1
            # 使用更保守的界限系数，减小置信区间
            lower_bound = q1 - 1.2 * iqr  
            upper_bound = q3 + 1.2 * iqr
            filtered_similarities = similarities[(similarities >= lower_bound) & 
                                               (similarities <= upper_bound)]
            
            # 如果过滤后样本太少，使用原始数据
            if len(filtered_similarities) < max(20, len(similarities) * 0.1):  # 至少保留10%的样本或20个样本
                print(f"第{layer_idx}层: 异常值过滤后样本太少，使用原始数据")
                filtered_similarities = similarities
            else:
                print(f"第{layer_idx}层: 移除了{len(similarities) - len(filtered_similarities)}/{len(similarities)}个异常值")
            
            # 更新结果中的相似度数据
            result['filtered_similarities'] = filtered_similarities
            result['original_sample_count'] = len(similarities)
            result['filtered_sample_count'] = len(filtered_similarities)
            
            # 计算更精确的置信区间
            n = len(filtered_similarities)
            mean = np.mean(filtered_similarities)
            std = np.std(filtered_similarities, ddof=1)  # 使用无偏估计
            
            # 根据样本大小选择合适的置信区间计算方法
            if n >= 30:  # 大样本使用z分布
                from scipy.stats import norm
                z = norm.ppf(0.975)  # 95%置信区间
                margin = z * (std / np.sqrt(n))
            else:  # 小样本使用t分布
                from scipy.stats import t
                t_val = t.ppf(0.975, n-1)  # 95%置信区间，自由度为n-1
                margin = t_val * (std / np.sqrt(n))
            
            # 更新置信区间
            result['ci_lower'] = mean - margin
            result['ci_upper'] = mean + margin
            result['ci_width'] = 2 * margin
            result['ci_method'] = 'z-distribution' if n >= 30 else 't-distribution'
            
            # 与关键层比较
            p_values = {}
            
            for key_layer in key_layers:
                if key_layer != layer_idx and key_layer < len(self.layer_similarities):
                    key_result = self.layer_similarities[key_layer]
                    key_similarities = key_result.get('filtered_similarities', key_result['similarities'])
                    
                    # 对比层也进行同样的预处理（如果尚未处理）
                    if not isinstance(key_similarities, np.ndarray):
                        key_similarities = np.array(key_similarities)
                        
                    # 确保数据干净
                    key_median = np.nanmedian(key_similarities)
                    key_similarities = np.nan_to_num(key_similarities, nan=key_median,
                                                   posinf=np.nanmax(key_similarities[np.isfinite(key_similarities)]) if np.any(np.isfinite(key_similarities)) else 1.0,
                                                   neginf=np.nanmin(key_similarities[np.isfinite(key_similarities)]) if np.any(np.isfinite(key_similarities)) else -1.0)
                    
                    # 确保样本长度匹配 - 使用随机采样而不是截断，以避免偏差
                    min_len = min(len(filtered_similarities), len(key_similarities))
                    if min_len < 15:  # 提高最小样本要求
                        print(f"警告: 第{layer_idx}层与第{key_layer}层比较的样本数量不足({min_len})")
                        continue
                    
                    # 随机采样以匹配长度
                    np.random.seed(42)  # 设置随机种子以确保可重复性
                    if len(filtered_similarities) > min_len:
                        indices = np.random.choice(len(filtered_similarities), min_len, replace=False)
                        sim1 = filtered_similarities[indices]
                    else:
                        sim1 = filtered_similarities
                        
                    if len(key_similarities) > min_len:
                        indices = np.random.choice(len(key_similarities), min_len, replace=False)
                        sim2 = key_similarities[indices]
                    else:
                        sim2 = key_similarities
                    
                    # 检查数据是否满足参数检验假设 - 正态性检验
                    try:
                        from scipy.stats import shapiro
                        _, p_shapiro1 = shapiro(sim1)
                        _, p_shapiro2 = shapiro(sim2)
                        is_normal = p_shapiro1 > 0.05 and p_shapiro2 > 0.05
                    except Exception as e:
                        print(f"正态性检验失败: {e}")
                        is_normal = False
                        
                    # 根据数据特性选择合适的检验方法
                    if is_normal:
                        # 配对t检验
                        try:
                            t_stat, p_ttest = ttest_rel(sim1, sim2)
                            # 计算效应量 (Cohen's d) - 使用更精确的公式
                            d = (np.mean(sim1) - np.mean(sim2)) / np.sqrt(
                                ((len(sim1)-1)*np.var(sim1, ddof=1) + (len(sim2)-1)*np.var(sim2, ddof=1)) / 
                                (len(sim1) + len(sim2) - 2)
                            )
                            test_results = {
                                'test': 'ttest',
                                'statistic': float(t_stat),  # 确保可序列化
                                'p_value': float(p_ttest),
                                'effect_size': float(d),
                                'effect_size_type': "Cohen's d"
                            }
                        except Exception as e:
                            print(f"t检验失败: {e}, 使用非参数检验")
                            is_normal = False
                    
                    if not is_normal:
                        # Wilcoxon符号秩检验
                        try:
                            w_stat, p_wilcoxon = wilcoxon(sim1, sim2)
                            # 计算效应量 (r = Z / sqrt(N)) - 更精确的计算
                            from scipy.stats import norm
                            z = norm.ppf(1 - p_wilcoxon/2)  # 双侧检验
                            r = z / np.sqrt(2 * min_len)
                            test_results = {
                                'test': 'wilcoxon',
                                'statistic': float(w_stat),
                                'p_value': float(p_wilcoxon),
                                'effect_size': float(r),
                                'effect_size_type': "r (Z/sqrt(N))"
                            }
                        except Exception as e:
                            print(f"Wilcoxon检验失败: {e}, 使用Mann-Whitney U检验")
                            try:
                                from scipy.stats import mannwhitneyu
                                u_stat, p_mw = mannwhitneyu(sim1, sim2)
                                # 计算效应量 - 使用更准确的公式
                                r = 1 - (2 * u_stat) / (min_len * min_len)
                                test_results = {
                                    'test': 'mannwhitneyu',
                                    'statistic': float(u_stat),
                                    'p_value': float(p_mw),
                                    'effect_size': float(r),
                                    'effect_size_type': "r (effect size)"
                                }
                            except Exception as e:
                                print(f"所有统计检验都失败: {e}")
                                test_results = {
                                    'test': 'failed',
                                    'p_value': 1.0,
                                    'effect_size': 0.0
                                }
                    
                    p_values[f'vs_layer_{key_layer}'] = test_results
            
            result['statistical_tests'] = p_values
        
        # 添加多重校正 - 支持多种校正方法
        try:
            from statsmodels.stats.multitest import multipletests
            
            # 收集所有p值
            all_p_values = []
            p_value_mapping = []  # 存储p值的位置信息
            
            for i, result in enumerate(self.layer_similarities):
                if 'statistical_tests' in result:
                    for comparison, test_result in result['statistical_tests'].items():
                        if 'p_value' in test_result:
                            all_p_values.append(test_result['p_value'])
                            p_value_mapping.append((i, comparison))
            
            if all_p_values:
                # 应用多种校正方法
                methods = ['fdr_bh', 'bonferroni', 'holm']
                correction_results = {}
                
                for method in methods:
                    try:
                        _, corrected_pvals, _, _ = multipletests(all_p_values, method=method)
                        correction_results[method] = corrected_pvals
                    except Exception as e:
                        print(f"{method}校正失败: {e}")
                
                # 将校正后的p值分配回原结果
                for method, corrected_pvals in correction_results.items():
                    for idx, (result_idx, comparison) in enumerate(p_value_mapping):
                        self.layer_similarities[result_idx]['statistical_tests'][comparison][f'p_value_{method}'] = corrected_pvals[idx]
                
                print(f"已应用多重校正: {', '.join(correction_results.keys())}")
        except ImportError:
            print("警告: 未安装statsmodels，跳过多重校正")
        except Exception as e:
            print(f"多重校正失败: {e}")
    
    def compute_mmd(self, X, Y, kernel='gaussian', bandwidth=None, multi_scale=True):
        """
        计算Maximum Mean Discrepancy (MMD)，使用向量化计算提高效率
        
        Args:
            X, Y: 两个样本集
            kernel: 核函数类型
            bandwidth: 高斯核带宽
            multi_scale: 是否使用多尺度核
        """
        # 增强数值稳定性处理
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)  # 使用0替代极端值
        Y = np.nan_to_num(Y, nan=0.0, posinf=0.0, neginf=0.0)
        
        # 平衡样本大小，避免样本不平衡导致的偏差
        min_samples = min(len(X), len(Y))
        if min_samples < len(X):
            # 如果X样本较多，随机抽样使其与Y样本数量相同
            indices = np.random.choice(len(X), min_samples, replace=False)
            X = X[indices]
        elif min_samples < len(Y):
            # 如果Y样本较多，随机抽样使其与X样本数量相同
            indices = np.random.choice(len(Y), min_samples, replace=False)
            Y = Y[indices]
        
        print(f"MMD计算: 平衡后样本数 X={len(X)}, Y={len(Y)}")
        
        # 标准化输入数据以提高数值稳定性
        X_norm = np.linalg.norm(X, axis=1, keepdims=True)
        Y_norm = np.linalg.norm(Y, axis=1, keepdims=True)
        
        # 避免除以零
        X_norm = np.where(X_norm > 1e-8, X_norm, 1e-8)
        Y_norm = np.where(Y_norm > 1e-8, Y_norm, 1e-8)
        
        X = X / X_norm
        Y = Y / Y_norm
        
        if bandwidth is None:
            # 使用分层抽样确保带宽计算的代表性
            max_samples_per_group = 500  # 每组最多使用500个样本计算带宽
            
            # 分别从X和Y中抽样
            if len(X) > max_samples_per_group:
                idx_X = np.random.choice(len(X), max_samples_per_group, replace=False)
                X_sample = X[idx_X]
            else:
                X_sample = X
                
            if len(Y) > max_samples_per_group:
                idx_Y = np.random.choice(len(Y), max_samples_per_group, replace=False)
                Y_sample = Y[idx_Y]
            else:
                Y_sample = Y
            
            # 合并样本计算带宽
            combined = np.vstack([X_sample, Y_sample])
            
            # 使用更稳定的方法计算带宽
            from sklearn.metrics import pairwise_distances
            
            # 计算成对距离矩阵
            try:
                # 如果样本太多，使用随机子集计算距离
                if len(combined) > 1000:
                    indices = np.random.choice(len(combined), 1000, replace=False)
                    sample = combined[indices]
                    distances = pairwise_distances(sample)
                else:
                    distances = pairwise_distances(combined)
                
                # 提取非零距离并计算中位数
                distances_triu = distances[np.triu_indices_from(distances, k=1)]  # 只取上三角部分
                nonzero_distances = distances_triu[distances_triu > 0]
                
                if len(nonzero_distances) > 0:
                    bandwidth = np.median(nonzero_distances)
                else:
                    bandwidth = 1.0  # 默认值
            except Exception as e:
                print(f"带宽计算失败: {e}, 使用默认值")
                bandwidth = 1.0
                
            # 确保带宽不会太小，避免数值不稳定
            bandwidth = max(bandwidth, 1e-3)
            
            print(f"自动计算的带宽: {bandwidth:.6f}")
        
        # 使用向量化计算核矩阵，增加数值稳定性
        def gaussian_kernel_matrix(X, Y, bandwidth):
            # 计算欧氏距离的平方，使用数值稳定的方法
            X_sqnorms = np.sum(X**2, axis=1, keepdims=True)
            Y_sqnorms = np.sum(Y**2, axis=1, keepdims=True)
            XY = np.dot(X, Y.T)
            
            # 确保距离计算的数值稳定性
            dist_sq = np.maximum(X_sqnorms + Y_sqnorms.T - 2 * XY, 0.0)
            
            # 使用安全的指数计算，避免上溢和下溢
            gamma = 1.0 / (2 * bandwidth**2)
            # 限制指数参数的范围，避免数值溢出
            K = np.exp(-np.minimum(dist_sq * gamma, 30.0))
            return K
        
        if multi_scale:
            # 多尺度核：使用多个带宽
            bandwidths = [bandwidth * 0.5, bandwidth, bandwidth * 2.0]
            mmd_values = []
            
            for bw in bandwidths:
                K_XX = gaussian_kernel_matrix(X, X, bw)
                K_YY = gaussian_kernel_matrix(Y, Y, bw)
                K_XY = gaussian_kernel_matrix(X, Y, bw)
                
                # 使用无偏估计器计算MMD
                n = len(X)
                m = len(Y)
                
                # 移除对角线元素（自身与自身的相似度）
                K_XX_sum = (np.sum(K_XX) - np.sum(np.diag(K_XX))) / (n * (n - 1))
                K_YY_sum = (np.sum(K_YY) - np.sum(np.diag(K_YY))) / (m * (m - 1))
                K_XY_sum = np.sum(K_XY) / (n * m)
                
                # 计算无偏MMD估计
                mmd_squared = K_XX_sum + K_YY_sum - 2 * K_XY_sum
                mmd_values.append(max(0, mmd_squared))
            
            # 返回多尺度MMD的平均值
            return np.sqrt(np.mean(mmd_values))
        else:
            # 单尺度MMD
            K_XX = gaussian_kernel_matrix(X, X, bandwidth)
            K_YY = gaussian_kernel_matrix(Y, Y, bandwidth)
            K_XY = gaussian_kernel_matrix(X, Y, bandwidth)
            
            # 使用无偏估计器计算MMD
            n = len(X)
            m = len(Y)
            
            # 移除对角线元素（自身与自身的相似度）
            K_XX_sum = (np.sum(K_XX) - np.sum(np.diag(K_XX))) / (n * (n - 1))
            K_YY_sum = (np.sum(K_YY) - np.sum(np.diag(K_YY))) / (m * (m - 1))
            K_XY_sum = np.sum(K_XY) / (n * m)
            
            # 计算无偏MMD估计
            mmd_squared = K_XX_sum + K_YY_sum - 2 * K_XY_sum
            return np.sqrt(max(0, mmd_squared))
    
    def compute_feature_distribution_difference(self, n_permutations=1000):
        """
        计算特征分布差异 (MMD)
        """
        print("计算特征分布差异...")
        
        # 提取表示
        uyghur_texts = [pair['uyghur'] for pair in self.parallel_data]
        chinese_texts = [pair['chinese'] for pair in self.parallel_data]
        
        uyghur_representations = self.extract_sentence_representations(uyghur_texts)
        chinese_representations = self.extract_sentence_representations(chinese_texts)
        
        results = []
        
        for layer_idx in tqdm(range(self.num_layers)):
            uy_vectors = uyghur_representations[layer_idx]
            zh_vectors = chinese_representations[layer_idx]
            
            # 增强数值稳定性处理
            uy_vectors = np.nan_to_num(uy_vectors, nan=0.0, posinf=0.0, neginf=0.0)
            zh_vectors = np.nan_to_num(zh_vectors, nan=0.0, posinf=0.0, neginf=0.0)
            
            # 检测和移除异常值
            def remove_outliers(vectors, threshold=3.0):
                # 计算每个向量的L2范数
                norms = np.linalg.norm(vectors, axis=1)
                # 计算范数的均值和标准差
                mean_norm = np.mean(norms)
                std_norm = np.std(norms)
                # 识别非异常值的索引
                if std_norm > 1e-10:  # 避免除以接近零的值
                    normal_idx = np.where(np.abs(norms - mean_norm) <= threshold * std_norm)[0]
                else:
                    normal_idx = np.arange(len(vectors))
                return vectors[normal_idx], normal_idx
            
            # 移除异常值
            uy_vectors_clean, uy_normal_idx = remove_outliers(uy_vectors)
            zh_vectors_clean, zh_normal_idx = remove_outliers(zh_vectors)
            
            print(f"第{layer_idx}层: 维吾尔语移除了{len(uy_vectors) - len(uy_vectors_clean)}/{len(uy_vectors)}个异常值")
            print(f"第{layer_idx}层: 汉语移除了{len(zh_vectors) - len(zh_vectors_clean)}/{len(zh_vectors)}个异常值")
            
            # 如果移除异常值后样本太少，则使用原始数据
            if len(uy_vectors_clean) < 10 or len(zh_vectors_clean) < 10:
                print(f"第{layer_idx}层: 异常值移除后样本太少，使用原始数据")
                uy_vectors_clean = uy_vectors
                zh_vectors_clean = zh_vectors
            
            # PCA降维（动态确定组件数）
            combined_vectors = np.vstack([uy_vectors_clean, zh_vectors_clean])
            n_samples, n_features = combined_vectors.shape
            
            # 确定PCA组件数：不超过样本数和特征数的最小值，且不超过50
            max_components = min(n_samples, n_features, 50)
            
            # 数据标准化，提高PCA稳定性
            from sklearn.preprocessing import StandardScaler
            
            if n_features > max_components and max_components > 1:
                print(f"第{layer_idx}层: 原始维度{n_features}, PCA降维到{max_components}")
                try:
                    # 先标准化数据
                    scaler = StandardScaler()
                    combined_scaled = scaler.fit_transform(combined_vectors)
                    uy_scaled = scaler.transform(uy_vectors_clean)
                    zh_scaled = scaler.transform(zh_vectors_clean)
                    
                    # 使用增量PCA处理大规模数据
                    if n_samples * n_features > 10**7:  # 如果数据规模很大
                        from sklearn.decomposition import IncrementalPCA
                        print(f"使用增量PCA处理大规模数据")
                        batch_size = min(n_samples, 1000)  # 批处理大小
                        pca = IncrementalPCA(n_components=max_components, batch_size=batch_size)
                    else:
                        from sklearn.decomposition import PCA
                        pca = PCA(n_components=max_components)
                    
                    # 拟合PCA
                    pca.fit(combined_scaled)
                    
                    # 转换数据
                    uy_vectors = pca.transform(uy_scaled)
                    zh_vectors = pca.transform(zh_scaled)
                    
                    # 计算并显示解释方差
                    explained_var = pca.explained_variance_ratio_.sum()
                    print(f"解释方差比: {explained_var:.3f}")
                    
                    # 如果解释方差太低，可能表明PCA不适合这些数据
                    if explained_var < 0.5:
                        print(f"警告: PCA解释方差较低，可能不适合这些数据")
                        
                except Exception as e:
                    print(f"PCA降维失败: {e}")
                    print(f"跳过第{layer_idx}层的PCA降维")
                    # 使用原始数据
                    uy_vectors = uy_vectors_clean
                    zh_vectors = zh_vectors_clean
            else:
                print(f"第{layer_idx}层: 维度{n_features}, 跳过PCA降维")
                uy_vectors = uy_vectors_clean
                zh_vectors = zh_vectors_clean
            
            # 计算MMD
            mmd_value = self.compute_mmd(uy_vectors, zh_vectors)
            
            # 置换检验
            combined = np.vstack([uy_vectors, zh_vectors])
            n_uy = len(uy_vectors)
            
            # 限制置换次数，对于大数据集
            actual_permutations = min(n_permutations, 1000)
            if actual_permutations < n_permutations:
                print(f"数据集较大，将置换次数从{n_permutations}减少到{actual_permutations}")
            
            permutation_mmds = []
            np.random.seed(42)
            
            for _ in range(actual_permutations):
                # 随机打乱标签
                indices = np.random.permutation(len(combined))
                perm_uy = combined[indices[:n_uy]]
                perm_zh = combined[indices[n_uy:]]
                
                perm_mmd = self.compute_mmd(perm_uy, perm_zh)
                permutation_mmds.append(perm_mmd)
            
            # 计算p值
            p_value = np.mean(np.array(permutation_mmds) >= mmd_value)
            
            results.append({
                'layer': layer_idx,
                'mmd': mmd_value,
                'p_value': p_value,
                'permutation_mmds': permutation_mmds
            })
        
        self.mmd_results = results
        return results
    
    def visualize_results(self, save_path=None):
        """
        可视化结果
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        plt.rcParams['font.sans-serif'] = ['SimHei']  # 支持中文
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. 层间相似度曲线
        ax1 = axes[0, 0]
        layers = [r['layer'] for r in self.layer_similarities]
        mean_sims = [r['mean_cosine'] for r in self.layer_similarities]
        ci_lowers = [r['ci_lower'] for r in self.layer_similarities]
        ci_uppers = [r['ci_upper'] for r in self.layer_similarities]
        
        ax1.plot(layers, mean_sims, 'b-o', linewidth=2, markersize=6, label='平均余弦相似度')
        ax1.fill_between(layers, ci_lowers, ci_uppers, alpha=0.3, label='95% 置信区间')
        ax1.set_xlabel('层数')
        ax1.set_ylabel('平均余弦相似度')
        ax1.set_title('层间相似度变化')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. MMD变化曲线
        ax2 = axes[0, 1]
        mmd_layers = [r['layer'] for r in self.mmd_results]
        mmd_values = [r['mmd'] for r in self.mmd_results]
        
        ax2.plot(mmd_layers, mmd_values, 'r-o', linewidth=2, markersize=6)
        ax2.set_xlabel('层数')
        ax2.set_ylabel('MMD值')
        ax2.set_title('特征分布差异(MMD)变化')
        ax2.grid(True, alpha=0.3)
        
        # 3. 相似度分布直方图（选择几个关键层）
        ax3 = axes[1, 0]
        key_layers = [0, 6, 7, 12, self.num_layers - 1]
        colors = ['blue', 'green', 'red', 'purple', 'orange']
        
        for i, layer_idx in enumerate(key_layers):
            if layer_idx < len(self.layer_similarities):
                similarities = self.layer_similarities[layer_idx]['similarities']
                ax3.hist(similarities, bins=30, alpha=0.5, color=colors[i], 
                        label=f'第{layer_idx}层', density=True)
        
        ax3.set_xlabel('余弦相似度')
        ax3.set_ylabel('密度')
        ax3.set_title('关键层相似度分布')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. p值热力图
        ax4 = axes[1, 1]
        p_values = [r['p_value'] for r in self.mmd_results]
        
        # 创建p值的热力图数据
        p_matrix = np.array(p_values).reshape(1, -1)
        im = ax4.imshow(p_matrix, cmap='RdYlBu_r', aspect='auto')
        ax4.set_xlabel('层数')
        ax4.set_title('MMD置换检验p值')
        ax4.set_yticks([])
        ax4.set_xticks(range(0, len(p_values), max(1, len(p_values)//10)))
        ax4.set_xticklabels(range(0, len(p_values), max(1, len(p_values)//10)))
        
        # 添加颜色条
        plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self):
        """
        生成分析报告
        """
        # 相似度报告
        print("\n" + "="*50)
        print("层间相似度分析报告")
        print("="*50)
        
        similarity_df = pd.DataFrame([
            {
                'Layer': r['layer'],
                'Mean_Cosine': f"{r['mean_cosine']:.4f}",
                'Std': f"{r['std_cosine']:.4f}",
                '95%_CI': f"[{r['ci_lower']:.4f}, {r['ci_upper']:.4f}]"
            }
            for r in self.layer_similarities
        ])
        
        print(similarity_df.to_string(index=False))
        
        # 找到相似度最高和最低的层
        max_sim_layer = max(self.layer_similarities, key=lambda x: x['mean_cosine'])
        min_sim_layer = min(self.layer_similarities, key=lambda x: x['mean_cosine'])
        
        print(f"\n最高相似度: 第{max_sim_layer['layer']}层 ({max_sim_layer['mean_cosine']:.4f})")
        print(f"最低相似度: 第{min_sim_layer['layer']}层 ({min_sim_layer['mean_cosine']:.4f})")
        
        # MMD报告
        print("\n" + "="*50)
        print("特征分布差异(MMD)分析报告")
        print("="*50)
        
        mmd_df = pd.DataFrame([
            {
                'Layer': r['layer'],
                'MMD': f"{r['mmd']:.4f}",
                'P_value': f"{r['p_value']:.4f}",
                'Significant': '是' if r['p_value'] < 0.05 else '否'
            }
            for r in self.mmd_results
        ])
        
        print(mmd_df.to_string(index=False))
        
        # 显著性统计
        significant_layers = [r for r in self.mmd_results if r['p_value'] < 0.05]
        print(f"\n显著差异层数: {len(significant_layers)}/{len(self.mmd_results)}")
        
        # 找到MMD最大和最小的层
        max_mmd_layer = max(self.mmd_results, key=lambda x: x['mmd'])
        min_mmd_layer = min(self.mmd_results, key=lambda x: x['mmd'])
        
        print(f"最大MMD: 第{max_mmd_layer['layer']}层 ({max_mmd_layer['mmd']:.4f}, p={max_mmd_layer['p_value']:.4f})")
        print(f"最小MMD: 第{min_mmd_layer['layer']}层 ({min_mmd_layer['mmd']:.4f}, p={min_mmd_layer['p_value']:.4f})")

def main():
    """
    主函数
    """
    # 设置路径
    model_path = "/data_home/models/Qwen3-8B"
    data_path = "/home/zuohuaidong/新疆日报数据集处理/二轮训练/val.json"
    output_dir = "/home/zuohuaidong/QWEN_VAL/tanzhen/任务驱动/二轮实验/翻译"
    
    # 创建输出目录
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # 初始化分析器
    print(f"初始化模型: {model_path}")
    analyzer = MultilingualLayerAnalysis(
        model_name_or_path=model_path,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # 加载数据
    print(f"加载数据: {data_path}")
    analyzer.load_parallel_data(data_path, sample_size=None)
    
    # 执行分析
    print("开始层间相似度分析...")
    analyzer.compute_layer_similarities()
    
    print("开始特征分布差异分析...")
    analyzer.compute_feature_distribution_difference(n_permutations=1000)
    
    # 生成报告和可视化
    print("生成分析报告...")
    analyzer.generate_report()
    
    # 保存可视化结果
    viz_path = os.path.join(output_dir, 'multilingual_analysis_results.png')
    analyzer.visualize_results(save_path=viz_path)
    
    # 保存详细数据
    save_detailed_results(analyzer, output_dir)
    
    print(f"\n分析完成！所有结果已保存到: {output_dir}")

def save_detailed_results(analyzer, output_dir):
    """
    保存详细的分析结果
    """
    import os
    import pickle
    
    # 保存相似度结果为CSV
    similarity_df = pd.DataFrame([
        {
            'Layer': r['layer'],
            'Mean_Cosine': r['mean_cosine'],
            'Std_Cosine': r['std_cosine'],
            'CI_Lower': r['ci_lower'],
            'CI_Upper': r['ci_upper']
        }
        for r in analyzer.layer_similarities
    ])
    similarity_df.to_csv(os.path.join(output_dir, 'layer_similarities.csv'), index=False)
    
    # 保存MMD结果为CSV
    mmd_df = pd.DataFrame([
        {
            'Layer': r['layer'],
            'MMD': r['mmd'],
            'P_Value': r['p_value'],
            'Significant': r['p_value'] < 0.05
        }
        for r in analyzer.mmd_results
    ])
    mmd_df.to_csv(os.path.join(output_dir, 'mmd_results.csv'), index=False)
    
    # 保存原始数据
    with open(os.path.join(output_dir, 'raw_results.pkl'), 'wb') as f:
        pickle.dump({
            'similarities': analyzer.layer_similarities,
            'mmd_results': analyzer.mmd_results,
            'parallel_data_sample': analyzer.parallel_data[:10]  # 保存样本
        }, f)
    
    # 生成文本报告
    with open(os.path.join(output_dir, 'analysis_report.txt'), 'w', encoding='utf-8') as f:
        f.write("中维平行语料层间分析报告\n")
        f.write("="*50 + "\n")
        f.write(f"模型: Qwen3-8B\n")
        f.write(f"数据集: 新疆日报数据集\n")
        f.write(f"样本数量: {len(analyzer.parallel_data)}\n")
        f.write(f"模型层数: {len(analyzer.layer_similarities)}\n\n")
        
        # 相似度统计
        f.write("层间相似度统计:\n")
        f.write("-" * 30 + "\n")
        max_sim = max(analyzer.layer_similarities, key=lambda x: x['mean_cosine'])
        min_sim = min(analyzer.layer_similarities, key=lambda x: x['mean_cosine'])
        f.write(f"最高相似度: 第{max_sim['layer']}层 ({max_sim['mean_cosine']:.4f})\n")
        f.write(f"最低相似度: 第{min_sim['layer']}层 ({min_sim['mean_cosine']:.4f})\n\n")
        
        # MMD统计
        f.write("特征分布差异统计:\n")
        f.write("-" * 30 + "\n")
        significant_count = sum(1 for r in analyzer.mmd_results if r['p_value'] < 0.05)
        f.write(f"显著差异层数: {significant_count}/{len(analyzer.mmd_results)}\n")
        max_mmd = max(analyzer.mmd_results, key=lambda x: x['mmd'])
        min_mmd = min(analyzer.mmd_results, key=lambda x: x['mmd'])
        f.write(f"最大MMD: 第{max_mmd['layer']}层 ({max_mmd['mmd']:.4f})\n")
        f.write(f"最小MMD: 第{min_mmd['layer']}层 ({min_mmd['mmd']:.4f})\n")
    
    print(f"详细结果已保存到: {output_dir}")
    print("包含文件:")
    print("- layer_similarities.csv: 层间相似度数据")
    print("- mmd_results.csv: MMD分析结果") 
    print("- multilingual_analysis_results.png: 可视化图表")
    print("- analysis_report.txt: 文本报告")
    print("- raw_results.pkl: 原始分析数据")

if __name__ == "__main__":
    main()
